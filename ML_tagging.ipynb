{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "mount_file_id": "1Kb02F6VYagJPculdi6itOrumTG31H-rV",
      "authorship_tag": "ABX9TyMustAflDqi2/nWtcuyzXwV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arijguest/ML-tagging/blob/main/ML_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML-Tagging** A Python project to allow automatic tagging of scraped text content from sitemap(s) using OpenAI 4o-mini. User should change the prompt given to the LLM to best reflect their tagging needs."
      ],
      "metadata": {
        "id": "O3GTm4WZzJ_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IcWSeYCjSMby"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Setup and Dependencies\n",
        "# !pip install requests beautifulsoup4 pandas nltk openai==0.28 tqdm aiohttp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import prerequisites\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import openai\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from tqdm.asyncio import tqdm_asyncio"
      ],
      "metadata": {
        "id": "udNHUWn_SaS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cu9uQY8OBJXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Helper function to remove stop words\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])"
      ],
      "metadata": {
        "id": "tB40zDp0Td-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define User Inputs\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'USER_API_KEY'\n",
        "\n",
        "# Function to allow user to set sitemap(s) and output directory.\n",
        "sitemaps = input('Enter the sitemap URLs, comma-separated: ').split(',')\n",
        "output_directory = input('Enter the output directory: ')"
      ],
      "metadata": {
        "id": "XELHH5TFTr5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Crawl Sitemaps\n",
        "def crawl_sitemap(sitemap_url):\n",
        "    try:\n",
        "        response = requests.get(sitemap_url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'xml')\n",
        "        links = [loc.text for loc in soup.find_all('loc')]\n",
        "        # Filter out image URLs\n",
        "        links = [link for link in links if not link.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        return links\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching sitemap {sitemap_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "all_links = []\n",
        "for sitemap_url in tqdm(sitemaps, desc='Crawling sitemaps'):\n",
        "    all_links.extend(crawl_sitemap(sitemap_url))"
      ],
      "metadata": {
        "id": "hVhzKJisTw1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Scrape and Preprocess Webpage Content\n",
        "def scrape_body_text(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Attempt to find the main content area\n",
        "        main_content = soup.find(['main', {'id': 'main-content'}, {'class': 'content'}])\n",
        "        if not main_content:\n",
        "            # Fall back to the body tag if no specific main content area is found\n",
        "            main_content = soup.find('body')\n",
        "\n",
        "        if main_content:\n",
        "            text = main_content.get_text(separator=' ', strip=True)\n",
        "            filtered_text = remove_stopwords(text)\n",
        "            return filtered_text\n",
        "        else:\n",
        "            return \"No main content found.\"\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return \"Error fetching content.\""
      ],
      "metadata": {
        "id": "U4aHOK96T3Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def analyze_content(session, url):\n",
        "    \"\"\"Analyzes webpage content asynchronously using OpenAI's GPT-4o-mini model.\"\"\"\n",
        "    try:\n",
        "        async with session.get(url) as response:\n",
        "            body_text = await response.text()\n",
        "            soup = BeautifulSoup(body_text, 'html.parser')\n",
        "            # Attempt to find the main content area\n",
        "            main_content = soup.find(['main', {'id': 'main-content'}, {'class': 'content'}])\n",
        "            if not main_content:\n",
        "                # Fall back to the body tag if no specific main content area is found\n",
        "                main_content = soup.find('body')\n",
        "            if main_content:\n",
        "                text = main_content.get_text(separator=' ', strip=True)\n",
        "                filtered_text = remove_stopwords(text)\n",
        "            else:\n",
        "                filtered_text = \"No main content found.\"\n",
        "\n",
        "        if filtered_text.startswith('Error'):\n",
        "            return filtered_text\n",
        "\n",
        "        title = soup.title.string if soup.title else 'No Title'  # Get title\n",
        "        response = await openai.ChatCompletion.acreate(  # Use acreate for async\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Generate ONLY hyper-specific tags for this webpage, focusing only on \"\n",
        "                           f\"material/waste types and processing methods and specific site content. Output ONLY in #tag-name \"\n",
        "                           f\"format (e.g., #anaerobic-digestion). Prioritize unique keywords relevant specifically \"\n",
        "                           f\"to the title: \\\"{title}\\\" and content: \\\"{filtered_text}\\\". For example, tags like \"\n",
        "                           f\"#WEE-waste should not show up in the Liquid Waste page tags. Avoid general tags like \"\n",
        "                           f\"#sustainability, and beware each page will contain mention of all waste types - do not \"\n",
        "                           f\"include these unless specific to that pageâ€™s title. Do not provide any additional \"\n",
        "                           f\"commentary or explanations, just output the hyper-relevant tags to that page.\"\n",
        "            }],\n",
        "            max_tokens=5000\n",
        "        )\n",
        "        tags = response['choices'][0]['message']['content'].strip()\n",
        "        return tags\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing content from {url}: {e}\")\n",
        "        return \"Error analyzing content.\""
      ],
      "metadata": {
        "id": "fqm2QzmtUA2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Process and Store Data\n",
        "async def process_and_store_data(all_links, output_directory):\n",
        "    \"\"\"Processes URLs asynchronously, stores results, and displays progress.\"\"\"\n",
        "    results = []\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async for link in tqdm_asyncio(all_links, desc='Processing links'):\n",
        "            tags = await analyze_content(session, link)  # Call analyze_content here\n",
        "\n",
        "            async with session.get(link) as response:  # Use the same session\n",
        "                html_content = await response.text()\n",
        "                soup = BeautifulSoup(html_content, 'html.parser')\n",
        "                title = soup.title.string if soup.title else 'No Title'\n",
        "            results.append({'URL': link, 'Title': title, 'Tags': tags})\n",
        "\n",
        "    store_data(results, output_directory)  # Call store_data with the final results\n",
        "\n",
        "# Main Logic\n",
        "async def main():\n",
        "    await process_and_store_data(all_links, output_directory)\n",
        "\n",
        "# Execute the main function directly\n",
        "await main()  # Start the asynchronous processing"
      ],
      "metadata": {
        "id": "FkqExn5yUGbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "maNnMtfM1iCr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
